{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                  Data Collection\n",
    "\n",
    "Download audio recording from the following website: https://web.archive.org/web/20150417081759/http://www.911dispatch.com/tape-library/\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/hduser/audio_data/High"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to transcribe audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr # import speech recognition module \n",
    "from os import path\n",
    "\n",
    "# Function to transcribe audio using IBM speech-to-text API.\n",
    "def IBM(x):\n",
    "    audio_file = path.join(path.dirname(path.realpath(x)), x)\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = r.record(source)\n",
    "    IBM_USERNAME = \"ba305638-8b4b-479c-b6f9-6c3db42261e6\"  \n",
    "    IBM_PASSWORD = \"qicSAYXGLKLy\"\n",
    "    text = r.recognize_ibm(audio,username=IBM_USERNAME,password=IBM_PASSWORD)\n",
    "    file = open(\"High_good_ibm.txt\", \"a\")\n",
    "    y = str(x)\n",
    "    file.write(y + \"\\n\" + str(text) + \"\\n\")\n",
    "    file.close()\n",
    "    print('File Saved')\n",
    "    return \n",
    "\n",
    "'''Initialize google cloude with 'gcloud init' then login into the account '''\n",
    "\n",
    "# Function to transcribe using Google speech-to-text API.\n",
    "def google(x):\n",
    "    audio_file = path.join(path.dirname(path.realpath(x)), x)\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = r.record(source)\n",
    "    GOOGLE_CLOUD_SPEECH_CREDENTIALS = None\n",
    "    text = r.recognize_google_cloud(audio, credentials_json=GOOGLE_CLOUD_SPEECH_CREDENTIALS)\n",
    "    file = open(\"High_Good_google.txt\", \"a\")\n",
    "    y = str(x)\n",
    "    file.write(y + \"\\n\" + str(text) + \"\\n\")\n",
    "    file.close()\n",
    "    print('File Saved')\n",
    "    return \n",
    "\n",
    "# Function to transcribe using CMU ASR.\n",
    "def cmu(x):\n",
    "    audio_file = path.join(path.dirname(path.realpath(x)), x)\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio = r.record(source)\n",
    "    text = r.recognize_sphinx(audio)\n",
    "    file = open(\"High_Good_cmu.txt\", \"a\")\n",
    "    y = str(x)\n",
    "    file.write(y + \"\\n\" + str(text) + \"\\n\")\n",
    "    file.close()\n",
    "    print('File Saved')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call functions to transcribe audio\n",
    "IBM('1.flac')\n",
    "google('1.flac')\n",
    "cmu('1.flac')\n",
    "# File is saved in txt format in the working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and a csv file format with attribute name as IncidentID, Text and Class for each ASR output. \n",
    "\n",
    "Preprocess the dataset\n",
    "\n",
    "Pre-processing step\n",
    "1. Regular expression\n",
    "2. lower case\n",
    "3. stemming\n",
    "4. Remove stopwords\n",
    "\n",
    "Functions to pre-process text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import nltk modules for pre-processing\n",
    "\n",
    "import nltk # Import Natural Language Toolkit module\n",
    "import re # Import regular expression library\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "\n",
    "# create a set of pre-defined stopwords from nltk for speed \n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Update stopwords list\n",
    "stopwords.update(['911','like', 'name', 'okay',\n",
    "                'ok', 'coming', 'could', 'days', 'everyone',\n",
    "                'get', 'give', 'going', 'liked', 'say', 'th',\n",
    "                'still', 'vs','call','operator',\n",
    "                'phone','hello','nine', 'address','one','building'])\n",
    "\n",
    "\n",
    "# create a stemmer variable. Assign this variable snowballstemmer.\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# Set regular expression pattern to pattern variable\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "# Function for stemming\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    return stemmed\n",
    "\n",
    "# Wrapped function for tokenization, Regular_expression, Stop words removal and stemming \n",
    "def preprocess_transcripts(transcript,\n",
    "                    token_pattern = pattern,\n",
    "                    exclude_stopword=True,\n",
    "                    stem=True):\n",
    "    # stop words are not removed for word2vec average features vectors\n",
    "    token_pattern = re.compile(token_pattern, flags = 0 )\n",
    "    tokens = [x.lower() for x in token_pattern.findall(transcript)]\n",
    "    tokens_stemmed = tokens\n",
    "    if stem:\n",
    "        tokens_stemmed = stem_tokens(tokens, stemmer)\n",
    "    if exclude_stopword:\n",
    "        tokens_stemmed = [x for x in tokens_stemmed if x not in stopwords]\n",
    "\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # import modules\n",
    "import random\n",
    "\n",
    "class features(object):\n",
    "    \n",
    "    # Bag of words feature vectors. Binary occurence marker of features are used.\n",
    "    def bow(train, test, num_features):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer   \n",
    "        # import countvectorizer module\n",
    "        vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                     tokenizer = None,\n",
    "                                     binary = True,\n",
    "                                     preprocessor = None,\n",
    "                                     stop_words = None,\n",
    "                                     max_features = num_features)\n",
    "        # fit the training dataset\n",
    "        train_bw = vectorizer.fit_transform(train.map(lambda x: ' '.join(x)))\n",
    "        test_bw = vectorizer.transform(test.map(lambda x: ' '.join(x)))\n",
    "        print('Vectorized')\n",
    "        return train_bw, test_bw # return bag-of-words feature vectors\n",
    "    \n",
    "    # Tf-idf vectorization function\n",
    "    def tfidf(train, test, num_features):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        # Create unigrams\n",
    "        vectorizer = TfidfVectorizer(sublinear_tf=True,  \n",
    "                                 max_df=0.8, \n",
    "                                 min_df=2,\n",
    "                                 ngram_range=(1,2), \n",
    "                                 norm = 'l1',\n",
    "                                 max_features = num_features)\n",
    "        train_tf = vectorizer.fit_transform(train.map(lambda x: ' '.join(x)))\n",
    "        test_tf = vectorizer.transform(test.map(lambda x: ' '.join(x)))\n",
    "        print('Vectorized')\n",
    "        return train_tf, test_tf # return tfidf features\n",
    "    \n",
    "    \n",
    "    '''Class to create word2vec average vectors'''\n",
    "class word2vec(object):\n",
    "    # Function to average all of the word vectors in a given transcript\n",
    "    def AvgFeatures(words, model, num_features):\n",
    "        # initialize an empty numpy array\n",
    "        feature = np.zeros((num_features,),dtype=\"float32\")\n",
    "        xwords = 0.\n",
    "        \n",
    "        #index2word is a list od words in the model's vocabulary. convert it into set.\n",
    "        model_words_set = set(model.wv.index2word)\n",
    "        \n",
    "        # Loop over each word in the transcript and if that word is in the model's vocabulory, \n",
    "        # add its feature vector to the total in numpy array \n",
    "        for word in words:\n",
    "            if word in model_words_set:\n",
    "                xwords = xwords + 1.\n",
    "                feature = np.add(feature, model[word])\n",
    "        # Divide the features by the total number of words in the transcript.\n",
    "        avgfeatures = np.divide(feature, xwords)\n",
    "        return avgfeatures\n",
    "\n",
    "    # load pre-trained model\n",
    "    def load_model(trained_model):\n",
    "        from gensim.models import Word2Vec\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(trained_model, binary=True)\n",
    "        print('Model Loaded')\n",
    "        return model             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change directory to the dataset directory\n",
    "\n",
    "Import train and test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/hduser/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for pandas dataframe\n",
    "def pandas_DataFrame(filename):\n",
    "    import pandas as pd\n",
    "    return pd.DataFrame.from_csv(filename, sep=',' , encoding='utf-8')\n",
    "\n",
    "# import train dataset\n",
    "ibm = pandas_DataFrame('ibm.csv')\n",
    "cmu = pandas_DataFrame('cmu.csv')\n",
    "google = pandas_DataFrame('google.csv')\n",
    "\n",
    "# import test dataset\n",
    "test = pandas_DataFrame('google.csv')\n",
    "\n",
    "# print first five rows\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize dataset\n",
    "ibm = ibm.iloc[np.random.permutation(len(ibm))]\n",
    "cmu = cmu.iloc[np.random.permutation(len(cmu))]\n",
    "google = google.iloc[np.random.permutation(len(google))]\n",
    "\n",
    "test = test.iloc[np.random.permutation(len(test))]\n",
    "# print first five rows\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Create unigrams from train and test dataset\n",
    "Get list of unigrams from train and test dataset using map function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigrams of training dataset\n",
    "# IBM\n",
    "ibm['unigram'] = ibm['Text'].map(lambda x: preprocess_transcripts(x))\n",
    "\n",
    "# Google\n",
    "google['unigram'] = google['Text'].map(lambda x: preprocess_transcripts(x))\n",
    "\n",
    "# cmu\n",
    "cmu['unigram'] = cmu['Text'].map(lambda x: preprocess_transcripts(x))\n",
    "\n",
    "# unigrams of test dataset\n",
    "test['unigram'] = test['Text'].map(lambda x: preprocess_transcripts(x))\n",
    "\n",
    "# print unigrams of test dataset\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        Vectorize using Bag-of-words\n",
    "Create sparse matrix of feature vectors for train and test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating feature vectors for training and test dataset\n",
    "X_train_ibm, X_test = features.bow(ibm['unigram'] , test['unigram'], 1000) # using 1000 feature i.e. vocabulary list\n",
    "X_train_google, X_test = features.bow(google['unigram'], test['unigram'], 1000)\n",
    "X_train_cmu, X_test = features.bow(cmu['unigram'], test['unigram'], 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        Vectorize using tf-idf vectors\n",
    "Create spare matrix of tf-idf feature vectors for Google dataset                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 1000 features\n",
    "x_train_google_tf, X_test_tf = features.tfidf(google['unigram'], test['unigram'], 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        Take average word2vec features\n",
    "Create average feature vectors for Google transcript                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "# Loading takes about 10 mins depending upon the memory avaiable. \n",
    "# Download pre-trained word2vec model using: 'git clone https://github.com/mmihaltz/word2vec-GoogleNews-vectors' \n",
    "# in terminal\n",
    "import gensim\n",
    "model = word2vec.load_model('/home/hduser/model/GoogleNews-vectors-negative300.bin')\n",
    "num_features = 300 # take 300 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average vectors\n",
    "X_train_google_wv = word2vec.AvgFeatures(google, model, num_features )\n",
    "X_test_wv = word2vec.AvgFeatures(test, model, num_features )\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "# Normalise\n",
    "X_train_google_wv =normalize(X_train_google_wv)\n",
    "X_test_wv = normalize(X_test_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create targets\n",
    "y_train_ibm = ibm['Class'].values\n",
    "y_train_google = google['Class'].values\n",
    "y_train_cmu = cmu['Class'].values\n",
    "y_test = test['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    benchmark classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Sklearn classifier module\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Import sklearn metrics4\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import sys\n",
    "# Use for Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "labels = ['High', 'Medium', 'Low']\n",
    "\n",
    "def compare(clf):\n",
    "    print('=' * 80)\n",
    "    print('Training on IBM dataset')\n",
    "    clf.fit(X_train_ibm, y_train_ibm) # Fit the data for training\n",
    "    pred_ibm = clf.predict(X_test)\n",
    "    acc_ibm = metrics.accuracy_score(y_test, pred_ibm) # Get accuracy\n",
    "    f_ibm = f1_score(y_test, pred_ibm, average='weighted') # Get f-score\n",
    "    pre_ibm = precision_score(y_test, pred_ibm, average='weighted') # Get Precision score\n",
    "    re_ibm = recall_score(y_test, pred_ibm, average='weighted')  # Get Recall score\n",
    "    print('Accuracy on IBM:', acc_ibm)\n",
    "    print('F-score on IBM:', f_ibm)\n",
    "    print('Precision on IBM:', pre_ibm )\n",
    "    print('Recall on IBM:', re_ibm )\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('Training on Google dataset')\n",
    "    clf.fit(X_train_google, y_train_google) # Fit the data for training\n",
    "    pred_google = clf.predict(X_test)\n",
    "    acc_google = metrics.accuracy_score(y_test, pred_google) # Get accuracy\n",
    "    f_google = f1_score(y_test, pred_google, average='weighted')\n",
    "    pre_google = precision_score(y_test, pred_google, average='weighted')\n",
    "    re_google = recall_score(y_test, pred_google, average='weighted')  \n",
    "    print('Accuracy on Google:', acc_google)\n",
    "    print('F-score on Google:', f_google)\n",
    "    print('Precision on Google:', pre_google )\n",
    "    print('Recall on Google:', re_google )\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('Training on CMU dataset')\n",
    "    clf.fit(X_train_cmu, y_train_cmu) # Fit the data for training\n",
    "    pred_cmu = clf.predict(X_test)\n",
    "    acc_cmu = metrics.accuracy_score(y_test, pred_cmu) # Get accuracy\n",
    "    f_cmu = f1_score(y_test, pred_cmu, average='weighted')\n",
    "    pre_cmu = precision_score(y_test, pred_cmu, average='weighted')\n",
    "    re_cmu = recall_score(y_test, pred_cmu, average='weighted')  \n",
    "    print('Accuracy on CMU:', acc_cmu)\n",
    "    print('F-score on CMU:', f_cmu)\n",
    "    print('Precision on CMU:', pre_cmu)\n",
    "    print('Recall on CMU:', re_cmu )\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('Training on tfidf vectors')\n",
    "    clf.fit(x_train_google_tf, y_train_google) # Fit the data for training\n",
    "    pred_google_tf = clf.predict(X_test_tf)\n",
    "    acc_google_tf = metrics.accuracy_score(y_test, pred_google_tf) # Get accuracy\n",
    "    f_google_tf = f1_score(y_test, pred_google_tf, average='weighted')\n",
    "    pre_google_tf = precision_score(y_test, pred_google_tf, average='weighted')\n",
    "    re_google_tf = recall_score(y_test, pred_google_tf, average='weighted')  \n",
    "    print('Accuracy on Google:', acc_google_tf)\n",
    "    print('F-score on Google:', f_google_tf)\n",
    "    print('Precision on Google:', pre_google_tf )\n",
    "    print('Recall on Google:', re_google_tf )\n",
    "    \n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('Training on word2vec avg vectors')\n",
    "    clf.fit(X_train_google_wv, y_train_google) \n",
    "    pred_google_wv = clf.predict(X_test_wv)\n",
    "    acc_google_wv = metrics.accuracy_score(y_test, pred_google_wv) \n",
    "    f_google_wv = f1_score(y_test, pred_google_wv, average='weighted')\n",
    "    pre_google_wv = precision_score(y_test, pred_google_wv, average='weighted')\n",
    "    re_google_wv = recall_score(y_test, pred_google_wv, average='weighted')  \n",
    "    print('Accuracy on Google:', acc_google_wv)\n",
    "    print('F-socre on Google:', f_google_wv)\n",
    "    print('Precision on Google:', pre_google_wv)\n",
    "    print('Recall on Google:', re_google_wv)\n",
    "    print('-' * 80)\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    cls = clf_descr\n",
    "\n",
    "    return clf_descr, ibm_acc, google_acc, cmu_acc, acc_google_tf, acc_google_wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Train and test classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and test classifiers \n",
    "\n",
    "# Results from SVM\n",
    "results = []\n",
    "print('=' * 80)\n",
    "print(\"SVM\")\n",
    "results.append(compare(LinearSVC(penalty='l1', dual=False, \n",
    "                                   multi_class='ovr',\n",
    "                                   tol=1e-3)))\n",
    "# Results from AdaBoost\n",
    "print('=' * 80)\n",
    "print('AdaBoost')\n",
    "results.append(compare(AdaBoostClassifier(n_estimators=256, algorithm='SAMME.R')))\n",
    "\n",
    "# Results from Bernoulli Naive Bayes\n",
    "print('=' * 80)\n",
    "print(\"BernoulliNB\")\n",
    "results.append(compare(BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)))\n",
    "# Results from Logistic regression\n",
    "print('=' * 80)\n",
    "print('Logistic Regression')\n",
    "results.append(compare(LogisticRegression(solver='liblinear', \n",
    "                                            penalty='l1', \n",
    "                                            class_weight='balanced',\n",
    "                                            multi_class = 'ovr')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        Print Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print Graph of comparison of accuracy of different Feature Vectors from Google\n",
    "indices = np.arange(len(results))\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "clf_names, ibm_acc, google_acc, cmu_acc = results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, ibm_acc, .2, label=\"Bag Of Words\", color='maroon')\n",
    "plt.barh(indices + .3, google_acc, .2, label=\"tf-idf\",\n",
    "         color='green')\n",
    "plt.barh(indices + .6, cmu_acc, .2, label=\"word2vec\", color='blue')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "plt.xlabel('Accuracy')\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.2, i, c)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print Graph for Accuracy on ASR datasets\n",
    "indices = np.arange(len(results))\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "clf_names, ibm_acc, google_acc, cmu_acc = results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, ibm_acc, .2, label=\"Google\", color='maroon')\n",
    "plt.barh(indices + .3, google_acc, .2, label=\"IBM\",\n",
    "         color='green')\n",
    "plt.barh(indices + .6, cmu_acc, .2, label=\"CMU\", color='blue')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "plt.xlabel('Accuracy')\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.2, i, c)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
